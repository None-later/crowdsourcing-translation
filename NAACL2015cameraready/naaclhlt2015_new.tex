%
% File naaclhlt2015.tex
%
\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2015}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{graphicx}
\usepackage{hhline}
%\usepackage{xltxtra}
%\usepackage{polyglossia}
%\usepackage{tablefootnote}%[2014/01/26]
%\usepackage{algpseudocode}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{comment}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
 %\usepackage{algolyx}
%\setotherlanguage{urdu}
%\newfontfamily\urdufont[Script=Arabic,Language=Urdu,Scale=1.5]{Amiri}
%\usepackage{footnotebackref}[2012/07/01]



\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
%Quality Control vs. Cost Optimization in Crowdsourcing Translation%
\title{Cost Optimization in Crowdsourcing Translation:\\ Low cost translations made even cheaper}

\author{Mingkun Gao, Wei Xu \and Chris Callison-Burch\\
  Computer and Information Science Department\\
  University of Pennsylvania, Philadelphia, PA, USA\ \\
  {\tt \{gmingkun, xwe, ccb\}@seas.upenn.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Crowdsourcing makes it possible to create translations at low cost. We proposed two mechanisms to make this process even cheaper while maintaining high translation quality.
First, we introduce a translator reducing method that allows us to reduce cost by quickly identifying bad translators after they have translated only a few sentences.  This allows us to rank translators, so that we only re-hire good translators and so that we can select the best translations from among good candidates. Second, we develop a translation reducing method.  We train a linear model to evaluate the translation quality on a sentence-by-sentence basis, and fit a threshold between acceptable and unacceptable translations.   Unlike past work, which always paid for a fixed number of translations of each source sentence and then chose the best from among them, we can decide after seeing a single translation whether it is good enough or not. Our model based selection allows us to reduce cost by reducing the number of redundant translations that we solicit. 
Additionally, we show that costs associated with gold standard calibration data created by professional translators can be reduced by using single reference instead of multiple references. 
%Besides, we reduce the cost for professional translations by reducing the number of references to calibrate each non-professional translation. 
 \end{abstract}

\section{Introduction}

Crowdsourcing is a promising new mechanism for collecting large volumes of annotated data at low cost.  %Platforms like Amazon Mechanical Turk (MTurk) provide researchers with access to large groups of people, who can complete `human intelligence tasks' that are beyond the scope of current artificial intelligence. 
%Since statistical natural language processing benefits from increased amounts of labeled training data, 
Many NLP researchers have focused on creating speech and language data through crowdsourcing (for example,  \newcite{snow2008}, \newcite{callison-burch-dredze2010} and others).  One NLP application that has been the focus of crowdsourced data collection is statistical machine translation (SMT) which requires large bilingual sentence-aligned parallel corpora to train translation models.  Crowdsourcing's low costs has made it possible to hire people to create sufficient volumes of translation in order to train SMT systems (for example,   \newcite{zbib2013systematic},  \newcite{zbib2012machine},  \newcite{post2012constructing}, \newcite{ambati2010can}).

However, crowdsourcing is not perfect, and one of its most pressing challenges is how to ensure the quality of the data that is created by it.  Unlike in more traditional employment scenarios, where annotators are pre-vetted and their skills are clear, in crowdsourcing very little is known about the annotators.  They are not professional translators, and there are no built-in mechanisms for testing their language skills.  They complete tasks without any oversight. Thus, translations produced via crowdousrcing may be low quality.
Previous work has addressed this problem, showing that non-professional translators hired on Amazon Mechanical Turk (MTurk) can achieve professional-level quality, by soliciting multiple translations of each source sentence and then choosing the best translation \cite{zaidan-callisonburch:2011:ACL-HLT2011a}.

In this paper we focus on a different aspect of crowdsourcing from \newcite{zaidan-callisonburch:2011:ACL-HLT2011a}.  We attempt to achieve the same high quality while {\bf minimizing the associated costs}.  
%We do so by reducing the number of redundant translations that have to be created for each source segment.
We reduce the costs associated with both professional and non-professional translations. Professional translations are used as calibration data for crowdsourcing. We show that using a single reference is as effective as using multiple references.
%We reduce costs of buying professional translations using \textbf{only one} reference to calibrate data.  
To  reduce costs for non-professional translations, we propose two complementary methods: (1) We reduce the number of workers we hire, and retain only high quality translators by quickly identifying and filtering out workers who produce low quality translations. 
(2) We reduce the number of translations that we solicit for each source sentence. Instead of soliciting a fixed number of translations for each foreign sentence, we stop soliciting translations after we get an acceptable one.  We do so by building models to distinguish between acceptable translations and unacceptable ones.  

Our work stands in contrast with  \newcite{zaidan-callisonburch:2011:ACL-HLT2011a} who had no model of annotator quality, and who always solicited and paid for a fixed number of translations of each source segment. 
 
 In this paper we demonstrate that
 \begin{itemize}
 \item Workers can be ranked by their quality with high correlation against a gold standard ranking, using linear regression and a variety of features, or initially testing them using a small amount of calibration data with known professional translations.
 \item This ranking can be established after observing very small amounts of data (reaching $\rho$ of 0.88 after seeing the translations of only 20 sentences from each worker). Therefore, bad workers can be filtered out quickly.
 \item Our models can predict whether a given translation is acceptable with high accuracy, substantially reducing the number of redundant translations needed for every source segment.
 \item We can achieve a similar BLEU score as \newcite{zaidan-callisonburch:2011:ACL-HLT2011a} at half the cost using our translation reducing method.
 % and at 60\% of the non-professional translations' cost using model selection method.  
 \end{itemize} 
 
\section{Problem Setup}

We start with a corpus of source sentences to be translated, and we may solicit one or more translations for every sentence in the corpus.  Our goal is to assemble a single high quality translation for each source sentence while minimizing the associated cost. 

We study the data collected by \newcite{zaidan-callisonburch:2011:ACL-HLT2011a} through Amazon's Mechanical Turk. 
They hired Turkers to translate 1792 Urdu sentences 
from the  2009 NIST Urdu-English Open Machine Translation Evaluation set\footnote{LDC Catalog number LDC2010T23}. A total of 52 Turkers contributed translations. Turkers also filled out a survey about their language skills and their countries of origin. Each Urdu sentence was translated by 4 non-professional translators (the Turkers) and 4 professional translators hired by the LDC. The cost of for non-professional translation is  \$0.10 per sentence and the cost of professional translation is \$0.30 per word (or just over \$6 on average for the sentences in our corpus which have an average of 20.1 words). 

Following \newcite{zaidan-callisonburch:2011:ACL-HLT2011a}, we use BLEU \cite{papineni2002bleu} to gauge the quality of human translations.  We can compute the expected quality of professional translation by comparing each of the professional translators against the other 3.  This results in an average BLEU score of  42.38.  By comparison, the Turker translations score only 28.13 on average. Zaidan and Callison-Burch trained a MERT model to select one non-professional translation out of the four and pushed the quality of crowdsourcing translation to 39.06, closer to the expected quality of professional translation. They used a small amount of professional translations (10\%) as calibration data to estimate the goodness of the non-professional translation. The component costs of their approach are the 4 non-professional translations for each source sentence, and the professional translations for the calibration data.

Although Zaidan and Callison-Burch demonstrated that non-professional translation was significantly cheaper than professionals, we are interested in further reducing the costs.  This plays a role if we would like to assemble a large enough parallel corpus (on the order of millions of pairs of sentences) to train a statistical machine translation system. 
% \cite{ambati2010can,post2012constructing,zbib2012machine,zbib2013systematic}
Here, we introduce several methods for reduce the number of non-professional translations  while still maintaining high quality. 



\section{Estimating Translation Quality}

We use a linear regression model\footnote{We used WEKA package: \url{http://www.cs.waikato.ac.nz/ml/weka/}} to predict a quality score ($ score(t) \in R$) for an input translation $t$.
%\begin{align*}
\[ score(t) = \vec{w}  \cdot \vec f(t) \]
  %\end{align*}
  where $\vec{w}$ is the associated weight vector and $\vec f(t)$ is the feature vector of the translation $t$. 

We replicate the feature set used by \newcite{zaidan-callisonburch:2011:ACL-HLT2011a} in their MERT model:

\begin{itemize}
\item  Sentence-level features: 9 features based on language model, sentence length, edit distance to other translations. 
\item Worker-level features: 15 features based on worker's language ability, location and average sentence-level scores.
\item Ranking features: 3 features based on the judgments of monolingual English speakers ranking the translations form best to worst.
\item Calibration features: 1 feature based on the average BLEU score of translations provided by the same worker, which is computed against professional references.
%\item Bilingual features: 1 feature based on the word alignment probabilities according. 
\end{itemize}


We additionally introduce a new bilingual feature based on IBM Model 1. We align words between each candidate translation and its corresponding source sentence. The bilingual feature for a translation is the average of its alignment probabilities. In Figure \ref{biexample1}, we show how the bilingual feature allows us to distinguish between a valid translation (top) and an invalid/spammy translation (bottom).

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{bilingualexample/example.png}
  \caption{Bilingual feature example of two crowdsourcing translations for a sentence in Urdu. The numbers are alignment probabilities for each aligned word.The bilingual feature is the average of these probabilities, thus 0.240 for the good translation and 0.043 for the bad translation.
}
    \label{biexample1}
\end{figure}

%\newcite{zaidan-callisonburch:2011:ACL-HLT2011a} used all 4 professional translations in computing the calibration features and training labels. We will show in the experiments that using \textbf{only one} professional translation as reference is as sufficient. 

\section{Reducing the Number of Translations}
The first mechanism that we introduce to optimize cost is one that reduces the number of translations.  Our goal is to recognize when we have got a good translation of a source sentence and to immediately stop purchasing additional  translations of that sentence.  The crux of this method is to decide whether a translation  is `good enough,' in which case we do not gain any benefit from  paying for another redundant translation.  

Our translation reduction method allows us to set an empirical definition of `good enough'.  We introduce a parameter  of the model  ($\delta$) that allows us to set how much degradation in translation quality is allowable, when we compare against selecting the best translation from the full set of redundant translations.  For instance, we may fix $\delta$ at 95\%, meaning that the BLEU score should not drop below 0.95 of the estimated BLEU score using the full set of non-professional translations.   We train a model to search for a threshold between acceptable and unacceptable translations ($\theta$) for a specific value of $\delta$. 

For a new translation, our model scores it, and if the its score is higher than $\theta$, then we do not solicit another translation. Otherwise, we continue to solicit translations.  Algorithm \ref{modelselection} details the process of model training and searching for $\theta$. 
\begin{algorithm} [h!]
\caption{}\label{modelselection}
\textbf{Input}: $\delta$, the allowable deviation from the expected upper bound on BLEU score (using all redundant translations); a training set $S = \{(x^{s}_{i,j},y^{s}_{i,j})_{j=1}^{4}\}_{i=1}^n$ and a validation set $V = \{(x^{v}_{i,j},y^{v}_{i,j})_{j=1}^{4}\}_{i=1}^n$ where $x_{i,j}$ is the feature vector for the jth translation of the source sentence $s_{i}$ and $y_{i,j}$ is the label for $x_{i,j}$.\\
\textbf{Output}: $\theta$, the threshold between acceptable and unacceptable translations; m(x), a linear regression model. 
\begin{algorithmic}[1]
\State \textbf{initialize} $\theta \leftarrow 0$,$m(x)\leftarrow \emptyset$ 
\State $m'(x)\leftarrow$ train a linear regression model on T
\State $maxbleu \leftarrow$ use $m'(x)$ select best translations for each $s_i \in T$ and record the highest model predicted BLEU score
\State $upperbound \leftarrow$ set an upper-bound BLEU score empirically
\While {$\theta \neq maxbleu $} 
\For {$i \leftarrow 1$ to $n$}
\For {$j \leftarrow 1$ to $4$}
\If {$m'(x^{v}_{i,j}) > \theta \wedge j < 4 $}
             select $x^{v}_{i,j}$ for $s_i$ and \textbf{break}
\EndIf
\If {$ j == 4 $} select $x^{v}_{i,j}$ for $s_i$
\EndIf
\EndFor
\EndFor
\State $q \leftarrow$ calculate translation quality for V
\If {$q > \delta \cdot upperbound$} \textbf{break}
\Else \text{  } $\theta = \theta + stepsize$
\EndIf
\EndWhile
\State $m \leftarrow$ train a linear regression model on $S \cup V$
\State \textbf{Return}: $\theta$ and model $m(x)$
%\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Experiment}
 We divide data into training set(10\%), validation set(10\%) and testing set(80\%). Each sample in training and validating set is labeled and calibrated by BLEU score calculated against \textbf{only one} reference. The step we set to sweep $\theta$ in validating process is 0.01 and the $upperbound$ is set to be 0.41 empirically. We vary the value of $\delta$ from 90\% to 100\% and the results we reported are based on five-fold cross validation.

\subsubsection{Baselines} We set a competitive method to compete with, which is revised from the framework of translation reducing mechanism with two different points: (1) we label and calibrate each sample in training and validating set using BLEU computed against four references; (2) we select translation from all candidates for each source sentence. We get a surprisingly high BLEU score of 40.13 using this method with a high cost over \$9,000(\$716.80 + \$8,644.6 for 20\% calibration). In addition, we set the random selection baseline and the corresponding BLEU score is 29.56.

\subsubsection{Translation reducing method} Table \ref{orderanother} shows the results for translation reducing method. We get comparable translation quality against our competing method with a much lower cost. If we set $\delta$ as 0.95, comparing two method, the difference in translation quality is 1.7 and for each source sentence, we almost avoid paying one redundant translation. The cost is:
 \begin{align*}
  C& = \frac{\alpha}{4} \cdot \frac{2}{10} N_{p}\cdot C_{p}  + \frac{3.12}{4} \beta\cdot \frac{8}{10} N_{np} \cdot C_{np}\\
   & = 2,608.43 (\$)
 \end{align*}
 which is 52\% of the original total cost and  28\% of the cost of our competing method.
 \begin{table}[h]
 \center
\begin{tabular}{c|c|c}
\hline
$\delta$(\%) & BLEU Score & \# of Trans. \\ \hhline{===}
90    & 37.04      & 2.11             \\
91    & 37.20      & 2.19             \\
92    & 37.77      & 2.54             \\
93    & 37.79      & 2.61             \\
94    & 38.31      & 3.07             \\
95    & 38.43      & 3.12             \\
96    & 38.65      & 3.44             \\
97    & 38.71      & 3.39             \\
98    & 39.17      & 3.88             \\
99    & 39.32      & 3.99             \\
100   & 39.36      & 3.99             \\ \hline
\end{tabular}
\caption{The relation among the $\delta$ (the proportion of the BLEU score's upper bound),the BLEU score for translations selected by models from partial sets and the averaged size of translation candidates set for each source sentence (\textit{\# of Trans}).  }
    \label{orderanother}
\end{table}
% \begin{table*}[t]
% \center
%\begin{tabular}{|c|c|c|c|}
%\hline
%Source                                   & References                         & Translations                         & Quality \\ \hline
%\multirow{4}{*}{فرانس کی تجویز کی حمایت} & Support of France's Recommendation & France has supported the proposal.   & 0.342   \\ \cline{2-4} 
%                                         & Support for the Proposal of France & Supporting the French proposal       & 0.630   \\ \cline{2-4} 
%                                         & French Proposal Endorsed           & France suggestion was appriciatable. & -0.014  \\ \cline{2-4} 
%                                         & French Proposal Supported          & defending the thinking of France.    & 0.269   \\ \hline
%\end{tabular}
%\caption{An example showing how to reduce redundant translations using the  model and the threshold. For each source sentence, we solicit 4 references and 4 non-professional translations. The value of 'Quality' is the \textbf{ model predicted score} for each translation which is different from the BLEU score. In this example, \textit{delta} is \%95, and the threshold telling apart acceptable and unacceptable translations is 0.35. Translations listed from top to the bottom are in the chronological order from the earliest submitted one to the latest submitted one. Since the first translation's quality value is lower that the threshold, we need to solicit another one.  Knowing that the second translation's quality value is higher than the threshold,  we stop soliciting other translations for this source sentence so that we avoid collect redundant translations and reduce cost.}
%\label{texample}
%\end{table*}


\section{Choosing Better Translators}

The second mechanism that we use to optimize cost is to reduce the number of non-professional translators that we hire.  Our goal is to quickly identify whether Turkers are good or bad translators, so that we can continue to hire only the good translators and stop hiring the bad translators after they are identified as such. 
%
Before presenting our method, we first demonstrate that Turkers produce consistent quality translations over time.

%reducing the number of translators by quickly identifying bad workers and filtering them out. We achieve high translation quality and avoid hiring bad workers continually after having detected their bad performance.

\subsection{Turkers' behavior in translating sentences}



\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{WorkerPerf/wp.pdf}
  \caption{A time-series plot of all of the translations produced by Turkers (identified by their WorkerID serial number). Turkers are sorted with the best translator at the top of the y-axis. Each tick represent a single translation and dark color means better quality.
}
    \label{fworkerperf}
\end{figure}

Do Turkers produce good (or bad) translations consistently or not? Are some Turkers  consistent and others not? We used the professional translations as a gold-standard to analyze the individual Turkers, and we found that most Turkers' performance stayed surprisingly consistent over time. 

Figure \ref{fworkerperf} illustrates the consistency of workers' quality by plotting quality of their individual translations on a timeline. The translation quality is computed based on the BLEU against professional translations. Each tick represent a single translation and depicts the BLEU score using two colors. The tick is black if its BLEU score is higher than the median and it is light grey otherwise. Good translators tend to produce consistently good translations and bad workers rarely produce good translations.

%Since workers' performance is consistent, workers' rankings are sufficiently accurate to reflect the quality of translations provided by them and we can select the translation which is provided by the worker with the best rank. We propose two approaches to rank workers from a small portion of work they submitted, filter out bad workers and select the best translation from translations provided by  surviving workers.

Next, we introduce two approaches to rank workers using a small portion of work they submitted.  Our goal is to filter out bad workers, and to select the best translation from translations provided by the remaining workers.

\subsection{Automatically Ranking Translators}


%We present translator reducing methods to compute ranks of workers from a small portion of work they submitted. We filter out bad workers and select the best translation from translations provided by  surviving workers.
%The consistency of workers' performance and the comparability of translation qualities between selecting translations by workers' ranks and selecting translations by models  are two preconditions guarantee this mechanism works.


%They are both very 'cheap' since we only use a small portion of professional translations and avoid hiring bad workers after we get workers' ranking.  
% and evaluate the translation quality by calculating the BLEU score against references.
\paragraph{Ranking workers using a model}
We use the linear regression model to score each translation and rank workers by their model predicted performance.  
%For each source sentence $s_i$, we have a translation set $T = \{ t_{i,1}, t_{i,2},t_{i,3}, t_{i,4}\}$. 
The model predicted score for translation $t$ is defined as $M(t)$. The model predicted performance of the worker $w$ is:
\begin{align*}
Performance(w) = \frac{\sum_{t \in T_{w}} M (t)}{|T_{w}|}
 \end{align*}
 where $T_{w} $ is the set of translations completed by $w$. 
After we rank workers, we keep top workers in the list and select translation provided by the worker with best rank among top workers.

\paragraph{Ranking workers using their first k translations}
Rather than using a model to rank workers, we take the first few translations provided by each Turker  and compare them to the professional translations of those sentences. We rank workers based on this gold standard data and discard bad workers.





\subsection{Experiments}
We report ranking's correlation to gold standard ranking and translation quality for both two methods.

\subsubsection{Baselines}
We evaluate ranking quality in Spearman's correlation($\rho$) compared with the gold standard ranking of workers. We score each Turker based on the average BLEU score of all his/her translations against professional references and we rank Turkers by their scores. We use the gold standard ranking as the ranking oracle and the upper-bound correlation is 1. In addition, we set two baselines for ranking correlation($\rho$) for our proposed approaches. For the first baseline,  we choose the MERT\cite{och2003minimum} baseline, which achieves a correlation of  0.67 when trained on ranking features. This is the highest correlation that  MERT achieves across all feature sets. The second baseline is a simpler baseline that reserves 10\% of the data for calibration, and computes a ranking of translators based on their BLEU scores against professionals over this calibration set, the correlation reaches 0.68.  

For translation quality evaluation, we set the gold standard ranking selection method as the oracle method, in which we select translation provided by the worker with best rank in gold standard ranking, and the BLEU score achieved is denoted as $B_{gold}$.
Besides, we set the random selection method as the baseline method which randomly select a translation from all candidates for each source sentence. 


\subsubsection{Ranking workers using a model}
We use 10\% of data to train a linear regression model to rank workers and select best translation by workers' ranking. Table \ref{lrresult} shows that our model achieves the highest BLEU score of 38.37 when trained on all features. If we calculate calibration feature and training sample label against only one reference rather than 4 references, we achieve a BLEU score of 37.52 with a correlation of 0.71 which is higher that two baselines. To  reduce cost, we retain only top 25\% workers and select the translation with the best rank provided by top workers. We achieve a BLEU score of 37.09 while $B_{gold}$ is 38.51 and the baseline is 29.95. The cost is:
 \begin{align*}
  C& = \frac{\alpha}{4} \cdot \frac{1}{10} N_{p}\cdot C_{p}  + \frac{\beta}{4} \cdot \frac{9}{10} N_{np} \cdot C_{np}\\
   & = 1,241.86 (\$)
 \end{align*}
 Thus, we can achieve a comparable translation quality by spending almost only 25\% of money with a quality lost of 1.53 in BLEU.
 
 
 
 \begin{table}[h]
 \center
\begin{tabular}{c|c|c}
\hline
Feature Set             & $\rho$  & BLEU  \\ \hhline{===}
(S)entence features     & 0.69 & 36.66 \\
(W)orker features       & 0.65 & 36.92 \\
(R)anking features      & 0.79 & 36.94 \\
Calibration features  & 0.79 & 38.27 \\
Calibration features* & \textbf{0.68} & \textbf{37.22} \\
S+W+R features          & 0.78 & 37.39 \\
S+W+R+Bilingual features        & 0.80 & 37.59 \\
All features            & \textbf{0.84} & \textbf{38.37} \\
All features*            & \textbf{0.71} & \textbf{37.52} \\ \hline
\end{tabular}
\caption{\label{lrresult} Spearman's correlation($\rho$) and translation quality of selecting best translations based on model-predicted workers' ranking for different feature sets. We don't filter out bad workers when selecting the best translation. * indicates that  we choose \textbf{only one} professional reference to calculate the BLEU score as calibration feature and the true label of a training sample while in other cases, we use 4 references to calculate BLEU score. }
\end{table}

\subsubsection{Ranking workers using their first k translations}
 Without using any model, we rank workers using their first k translations and select best translations based on rankings of the top 25\% workers. To evaluate this method, we created several test sets.  Each test set excluded any item that was used to rank the workers, or which did not have any translations from the top 25\% of workers according to our predicted rankings.  We therefore have \emph{different test sets} for each value of k.  This makes the results slightly more difficult to analyze than in normal experiments, although the trends are still clear.
Formally, we define the test set for first k sentences as $T_k$ and for each source sentence $s \in T_k$:
\begin{align*}
  \{ s \mid (C(s) \cap S_k = \emptyset)   \wedge (C(s) \cap S_w \neq \emptyset)    \}
\end{align*}
where $C(s)$ is the translating candidates set of the source sentence $s$, $S_k$ is the translation set consists of each worker's first k translations and $S_w$ is the translation set consists of translations provided by selected workers (some top ranking workers). 
\begin{table}[h]
\begin{tabular}{cc|cc}
\hline
\multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Proportion of \\ Calibration Data\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$\rho^{+}$\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$\rho^{*}$\end{tabular}} \\ \cline{1-2}
First k sentences                                 & Percentage                                 &                                                                         &                                                                        \\ \hhline{====}
1                                                 & 0.7\%                                      & 0.57                                                                    & 0.41                                                                   \\
2                                                 & 1.3\%                                      & 0.62                                                                    & 0.48                                                                   \\
3                                                 & 2.0\%                                      & 0.69                                                                    & 0.59                                                                   \\
4                                                 & 2.7\%                                      & 0.72                                                                    & 0.59                                                                   \\
5                                                 & 3.3\%                                      & 0.78                                                                    & 0.69                                                                   \\
6                                                 & 4.0\%                                      & 0.80                                                                    & 0.70                                                                   \\
7                                                 & 4.7\%                                      & 0.79                                                                    & 0.71                                                                   \\
8                                                 & 5.3\%                                      & 0.81                                                                    & 0.69                                                                   \\
9                                                 & 6.0\%                                      & 0.84                                                                    & 0.77                                                                   \\
10                                                & 6.6\%                                      & 0.84                                                                    & 0.76                                                                   \\
20                                                & 13.3\%                                     & 0.93                                                                    & 0.88                                                                   \\
30                                                & 19.9\%                                     & 0.96                                                                    & 0.93                                                                   \\
40                                                & 26.6\%                                     & 0.97                                                                    & 0.95                                                                   \\
50                                                & 33.2\%                                     & 0.98                                                                    & 0.95                                                                   \\ 
60                                                & 39.8\%                                     & 0.99                                                                    & 0.94                                                                  \\ \hline
\end{tabular}
\caption{\label{spearmansen} Spearman's Correlations for calibration data in different proportion. * indicates that the calibration is computed against \textbf{only one} reference while + indicates that the calibration is computed against 4 references.}
\end{table}
Table \ref{spearmansen} shows the results of Spearman's correlations for different value of K. Compared with 4-reference calibration, we achieve very strong correlation when calibrating the workers using one reference based on  the translations of their first 40 sentences. Even we only use the first 20 sentences to evaluate and rank workers, the correlation ($\rho^{*}$) is close to 0.90. Consequently, we can decide whether to continue to hire a worker in a very short time after analyzing the first k sentences ($k\le20$) provided by each worker. 

Figure \ref{firstksenbleu}  shows the BLEU score when we select the top 25\% workers from the ranking list based on the performance of first k sentences.
  As a comparison, we also plotted the  BLEU scores for random selection 
  and the BLEU score for selection based on the gold standard ranking($B_{gold}$).
 %Figure \ref{firstksenbleudiff}  shows the difference between BLEU scores we get in three different mechanisms in order to make the comparisons clear.
  As we increase the number of sentences we use to rank Turkers, the BLEU score we get from the ranking approaches $B_{gold}$. Surprisingly, we see that when only a small part of sentences (say 20 sentences) for each worker are used in ranking, the ranking list  is quite similar to the gold standard ranking list and the BLEU score is very close to the BLEU score get by gold standard ranking.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{senlevelselect/curve1.pdf}
  \caption{ The BLEU score for selecting the  best translation by the top 25\% Turkers' ranking based on the first k sentences (red line). The green line shows the BLEU score for selecting the best translation by the gold standard ranking. The dark blue line shows the BLEU score for selecting translation randomly.}
    \label{firstksenbleu}
\end{figure}
%
%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=\linewidth]{senlevelselect/curve2.pdf}
%  \caption{ The difference between BLEU scores reported from three different methods in Figure \ref{firstksenbleu}.}
%   \label{firstksenbleudiff}
%  \end{figure}

If we use the first 20 sentences to rank workers, the correlation is 0.88 and  the BLEU score achieved is 37.01 while  and $B_{gold}$ is 37.14. The difference between these two scores is 0.13 and the cost is: \\
  \begin{align*}
  C& = \frac{\alpha}{4} \cdot  (0.133 \cdot N_{p}) \cdot C_{p}  + \frac{\beta }{4}\cdot (0.867\cdot N_{np} ) \cdot C_{np}\\
   & = 1,592.53 (\$)
  \end{align*}  
We can achieve almost the same translation as the oracle method with only spending 32\% of the total cost. 


\section{Cost Analysis}

 We have introduced several ways of lowering the costs associated with crowdsourcing translations:
\begin{itemize}
\item We show that we can quickly identify bad translators, either with a model designed to rank them, or by ranking them by having them first translate a small number of sentences with gold standard translations. The cost savings for non-professionals here comes from not hiring bad workers.
\item After we have collected one translation of a source sentence, we consult a model that predicts whether its quality is sufficiently high or whether we should pay to have the sentence re-translated.  The cost savings for non-professionals here comes from reducing the number of redundant translations.
\item In both cases we need a some amount of professionally translated materials  to use as a gold standard for calibration. The cost savings for professionals come from reducing the referencing translations to calibrate each data sample. 
% The cost of these professional translations can dominate the cost of our models, so we experiment with how little we can get away with.
\end{itemize}
In all cases, there is a trade-off between lowering our costs and producing high quality translations.  Figure \ref{fbleucost} plots the cost versus the BLEU scores for the different configurations that we experimented with.

In Figure \ref{fbleucost}-(a) the increasing costs are a function of how many sentences we use to rank the translators.  Here we use no model, and simply rank the translators by their BLEU score against a small amount of gold standard data. The quality peaks at 37.9 BLEU after \$3,000.
%the return on investment is low after spending the first \$2,000 to get a BLEU of 35.6.
We are able to rank the translators with high accuracy and achieve a relative high BLEU score by paying for a comparatively small number of professional translations to use as calibration. From our experiments,  10-20 professionally translated sentences seems like a reasonable number. 

Figure\ref{fbleucost}-(b) uses a model to determine whether to purchase another translation.  
%Here the starting cost is high (nearly \$9,000) because the model requires a significant amount of professional translations in order to train the model and to determine the optimal threshold values for whether to solicit another translation. 
This model allows us to significantly improve the overall translation quality to a BLEU score of nearly 40, for a final cost of \$2,700.

%To emphasize the effectiveness of model selection approach, Figure \ref{fbleucost}-(c) plots the relationship between BLEU and non-professional component of the overall cost.  Past approaches to crowdsourcing translation always solicited 4 non-professional translations of every source sentence. The cost for translating our 1433 test sentences under this approach is \$573.44.  This produces the maximum BLEU score of 40.1.  Using our model to reduce the number of redundant translations, we can reduce the costs with mild degredation in translation quality.  We can cut the number of non-translations in half, and pay only \$286.72, while achieving a BLEU score of 37.6 (94\% of the maximum), or pay \$348.36,60.7\% of total non-professional translations' cost, for a BLEU of 38.5 (96\% of  the maximum).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{cost-bleu/pricecost.pdf}
  \caption{The Relationship between BLEU score and costs. In Figure (a), the red line shows the relationship between BLEU score and  the total costs (professional and non-professional) for the ranking based approach. 
  %The green line shows the corresponding translation quality for gold standard ranking selection measured in BLEU score. 
  Figure (b) shows the relationship between BLEU score and the total costs for model-based approach. 
  %Figure (c)  illustrates the relationship between BLEU score and non-professional costs for model based approach.
  }
    \label{fbleucost}
\end{figure}
\section{Related Work}
%Crowdsoucing has been widely used in data annotation or labeling to get a large amount of data at low cost. The relationship between the budget and the quality was discussed in previous work.

\newcite{sheng2008get}'s work on repeated labeling presents a way of solving the problems of uncertainty in labeling. Since we cannot always get high-quality labeled data samples with relatively low costs in reality, to keep the model trained on noisy labeled data having a high accuracy in predicting, \newcite{sheng2008get} proposed a framework for repeated-labeling that resolves the uncertainty in labeling via majority voting. The experimental results show that a model's predicting accuracy is improved even if labels in its training data are nosity and of imperfect quality.  As long as the integrated quality (the probability of the integrated labeling being correct) is higher than 0.5, repeated labeling benefits model training. 

\newcite{passonneau2013benefits} created a Bayesian model of annotation and they applied to the problem of word sense annotation. \newcite{passonneau2013benefits} also proposed an approach to detect and avoid spam workers. 
%They required workers to finish 20000 HITs and have a 98\% lifetime approval rating. 
They measured the performance of worker by comparing worker's labels to the current majority labels and worker with bad performance would be blocked. However, this approach suffered from 2 shortcomings: (1) Sometimes majority labels may not reflect the ground truth label. (2) They didn't figure out  how much data(HITs) is needed to evaluate a worker's performance. Although they could find the spam after the fact, it was a post-hoc analysis, so they had already paid for that worker and wasted the money.
%More closer the quality to 0.5, the more benefits obtained in model prediction.
%For one of our approaches for lowering the costs of crowdsourcing,  we train models to distinguish between acceptable and unacceptable translation candidates.  To do so,
% Candidates with BLEU scores higher than the threshold are acceptable and vice versa. We search for the threshold on the validating set that is leading to the BLEU within some delta of $S_{upper}$.  
%we sweep a threshold of BLEU values. The threshold between acceptable and unacceptable translations is fuzzy so there exists some uncertainty in labeling each data sample.  This is related to \cite{sheng2008get}'s work on repeated labeling, which presents a way of solving the problems of   uncertainty in labeling and selection of a threshold. In their work, they showed that for single-labeling examples, the labeling quality (the annotator's probability of producing a correct labeling) is critical to the model quality. The model prediction accuracy rises as the labeling quality increases. However, in reality, we cannot always get high-quality labeled data samples with relatively low costs. To keep the model trained on noisy labeled data having a high accuracy in predicting, \newcite{sheng2008get} proposed a framework for repeated-labeling that resolves the uncertainty in labeling via majority voting.  
%The experimental results show that a model's predicting accuracy is improved even if labels in its training data are nosity and of imperfect quality.  As long as the integrated quality (the probability of the integrated labeling being correct) is higher than 0.5, repeated labeling benefits model training. More closer the quality to 0.5, the more benefits obtained in model prediction.

%If we relax the condition on the uniform labeler quality and allow lablers to have different quality,  a new question arises: should we use the best individual labeler or should we combile the results from multiple labelers? According to the analysis,  it depends on how much the best labeler's quality is better the average quality of all labelers.

%Majority voting is an useful approach to improve the quality of corpus. However, it omits  the uncertain property of data samples' labels and loses the information about uncertainty in labels. To take advantage of the uncertainty, \newcite{sheng2008get} represent labeling uncertainty in probability. For each unlabeled data sample $x_{i}$, the \textit{multiplied examples} (ME) procedure takes the existing multi-label set  $L_i = \{y_{ij}\}$ as input, and for each label value $y_{ij} $ in the multiset, make a replica of $x_i$ which is labeled $y_{ij}$, and use the probability of that label value appearing in the multi-label set as the weight of that replica . We can use cost-sensitive learning method to train model on the modified data set. Experiment shows that ME strategy is better than than majority voting.

%In experiments, for each data set, 30\% data samples are held out as the test data, and the rest data is the "pool" from which we acquire unlabeled and labeled samples. When deciding the next data sample to be labeled, they use the generalized round robin strategy: selecting the data sample with the fewest labels. To make the selection more reasonable, they proposed the selective-repeated labeling method. For each data sample, $LU$ is defined as the label uncertainty which measures the labels' diversity on the data sample. Similarly, $MU$ is defined as the model uncertainty which measures the disagreement on models' prediction to the data sample. $LMU$ is defined as the label and model uncertainty which is the geometric average of $LU$ and $MU$. Instead of assigning the new label to  the data sample with fewest labels, they choose the data sample with the highest $LMU$ score and get benefits.

%Outline of related work of 'Benefits of a Model of Annotation'.\\
%1. Related Point: our work is trying to find the best translation from all candidates. Their work is trying to %select a label from the multi-label set.
%2. Different Point: our work is don't have labels but they have.

%A very important issue in natural language processing is data annotation. Hiring professional annotators is very expensive. As an alternative, collecting several annotation for each single data sample and pick the best label is  more economical.  In our work, we collected several translations for each source sentence and pick the best translation. Our work shares many goals in common with \newcite{passonneau2013benefits}, who created a Bayesian model of annotation, which they applied to the problem of word sense annotation. Rather than hiring professional annotators, which is very expensive, they hire non-expert annotators on Mechanical Turk.  They collected 20 to 25 word sense labels for each word. To decide which label to select for each word, and to compute the quality of the annotation, they proposed the probabilistic model using Bayes's rule. They calculated the product of the prior probability (the initial probability of being the observed label) and the conditional probability (the probability of being the observed label given the true label) and pick one label with the highest score. This sort of a probability estimate provides much more information about the corpus quality than previous methods, such as calculating inter-annotation agreement through Coehn's kappa score.  Kappa measures the agreement coefficient among annotators in a chance-adjusted fashion.  However, the method  only  reports how often annotators agree, but does not provide information about the quality of the corpus and the individual data sample.

%Although \newcite{passonneau2013benefits} collect word sense labels, which are a small, enumerable set, and we collect translation (which could be thought of as a kind of label, albeit a very complex one), there is a strong commonality in the goals of their word and the goals of our work.  Specifically, how can we use all the labels collected in order to select of the best label.  And how can we rank the annotators themselves.  For selecting the best label for word senses, majority voting is a direct and easy way to solve the problem, but the task is more complex for translation. 

%\newcite{passonneau2013benefits} also proposed an approach to detect and avoid spam workers. 
%They required workers to finish 20000 HITs and have a 98\% lifetime approval rating. 
%They measured the performance of worker by comparing worker's labels to the current majority labels and worker with bad performance would be blocked. However, this approach suffered from 2 shortcomings: (1) Sometimes majority labels may not reflect the ground truth label. (2) They didn't figure out  how much data(HITs) is needed to evaluate a worker's performance. Although they could find the spam after the fact, it was a post-hoc analysis, so they had already paid for that worker and wasted the money.  We attempt to identify poor workers as quickly as possible, in order to limit the amount of work that we solicit from them.
\newcite{lin2014re} examined the relationship between worker accuracy and budget in the context of using crowdsourcing to train a machine learning classifier.  They show that if the goal is to train a classifier on the labels, that the properties of the classifier will determine whether it is better to re-label data (resulting in higher quality labels) or get more single labeled items (of lower quality). They showed that classifiers with weak inductive bias  benefit more from relabeling, and that relabeling is more important when worker accuracy is low (barely higher than 0.5). 

\newcite{novotney2010cheap} showed a similar result for training an automatic speech recognition (ASR) system.  When creating training data for an ASR system, given a fixed budget. Their system's accuracy was higher when it is trained on more low quality transcription data compared to when it was trained on fewer high quality transcriptions.


\section{Conclusion}
In this paper, we propose two mechanisms to optimize cost: the translator reducing method and the translation reducing method. They have  different applicable scenarios. The translator reducing method is a very simple method without any model training. This approach is inspired by the intuition that workers' performance is consistent. The translator reducing method is suitable for crowdsourcing tasks which do not have specific requirements about the quality of the translations, or when the data collection is being performed by a requester who does not have sufficient background in machine learning in order to train a model, or when only very limited amounts of gold standard data are available.
The translation reducing method works if there exists a specific requirement that the quality control must reach a certain threshold, or when more data needs to be collected.  This model is most effective when reasonable amounts of pre-existing professional translations are available for setting the models threshold.  Its major cost reduction comes from dramatically reducing the amount of non-professional data to maintain the same quality.

\section*{Acknowledgments}

[ADD HERE]

\bibliographystyle{naaclhlt2015}
\bibliography{naaclhlt2015_new}

\end{document}